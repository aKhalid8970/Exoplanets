{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNE2FX0QoY7jSJzg/0TfiJO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aKhalid8970/Exoplanets/blob/main/Exoplanets1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v9ioNXnFMsX"
      },
      "outputs": [],
      "source": [
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTrain.csv'\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTest.csv'\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import  metrics\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, InputLayer\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling2D, BatchNormalization, MaxPooling1D\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "df_train = pd.read_csv('exoTrain.csv')\n",
        "df_train['LABEL'] = df_train['LABEL'] -1\n",
        "df_test = pd.read_csv('exoTest.csv')\n",
        "df_test['LABEL'] = df_test['LABEL'] - 1\n",
        "\n",
        "def plot_graphs(history, best):\n",
        "\n",
        "  plt.figure(figsize=[10,4])\n",
        "  # summarize history for accuracy\n",
        "  plt.subplot(121)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy across training\\n best accuracy of %.02f'%best[1])\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  # summarize history for loss\n",
        "  plt.subplot(122)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss across training\\n best loss of %.02f'%best[0])\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def analyze_results(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Helper function to help interpret and model performance.\n",
        "\n",
        "    Args:\n",
        "    model: estimator instance\n",
        "    X_train: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "    Input values for model training.\n",
        "    y_train : array-like of shape (n_samples,)\n",
        "    Target values for model training.\n",
        "    X_test: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "    Input values for model testing.\n",
        "    y_test : array-like of shape (n_samples,)\n",
        "    Target values for model testing.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    print(\"-------------------------------------------\")\n",
        "    print(\"Model Results\")\n",
        "    print(\"\")\n",
        "    print(\"Training:\")\n",
        "    if type(model) == keras.src.models.sequential.Sequential:\n",
        "      train_predictions = model.predict(X_train)\n",
        "      train_predictions = (train_predictions > 0.5)\n",
        "      cm = confusion_matrix(y_train, train_predictions)\n",
        "      labels = [0, 1]\n",
        "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "      fig = plt.figure()\n",
        "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
        "      plt.title('Confusion Matrix - Training Data')\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "      plt.show()\n",
        "    else:\n",
        "      plt.close()\n",
        "      ConfusionMatrixDisplay.from_estimator(model,X_train,y_train)\n",
        "      plt.show()\n",
        "\n",
        "    print(\"Testing:\")\n",
        "    if type(model) == keras.src.models.sequential.Sequential:\n",
        "      test_predictions = model.predict(X_test)\n",
        "      test_predictions = (test_predictions > 0.5)\n",
        "      cm = confusion_matrix(y_test, test_predictions)\n",
        "      labels = [0, 1]\n",
        "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "      fig = plt.figure()\n",
        "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
        "      plt.title('Confusion Matrix - Test Data')\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "      plt.show()\n",
        "    else:\n",
        "      ConfusionMatrixDisplay.from_estimator(model,X_test,y_test)\n",
        "\n",
        "X_train = df_train.drop('LABEL', axis=1)\n",
        "y_train = df_train['LABEL']\n",
        "X_test = df_test.drop('LABEL', axis=1)\n",
        "y_test = df_test['LABEL']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions that can run for the three augmentation functions that will be used, but not explroed in depth\n",
        "\n",
        "def smote(a,b):\n",
        "    model = SMOTE()\n",
        "    X,y = model.fit_resample(a, b)\n",
        "    return X,y\n",
        "\n",
        "def savgol(df1,df2):\n",
        "    x = savgol_filter(df1,21,4,deriv=0)\n",
        "    y = savgol_filter(df2,21,4,deriv=0)\n",
        "    return x,y\n",
        "\n",
        "def fourier(df1,df2):\n",
        "    X_train = np.abs(np.fft.fft(df1, axis=1))\n",
        "    X_test = np.abs(np.fft.fft(df2, axis=1))\n",
        "    return X_train,X_test\n",
        "\n",
        "def norm(df1,df2):\n",
        "    X_train = normalize(df1)\n",
        "    X_test = normalize(df2)\n",
        "    return X_train,X_test\n",
        "\n",
        "def robust(df1,df2):\n",
        "    scaler = RobustScaler()\n",
        "    X_train = scaler.fit_transform(df1)\n",
        "    X_test = scaler.transform(df2)\n",
        "    return X_train,X_test\n",
        "\n",
        "fourier_X_train, fourier_X_test = fourier(X_train, X_test)\n",
        "savgol_X_train, savgol_X_test = savgol(fourier_X_train, fourier_X_test)\n",
        "norm_X_train, norm_X_test = norm(savgol_X_train,savgol_X_test)\n",
        "robust_X_train, robust_X_test = robust(norm_X_train, norm_X_test)\n",
        "smote_X_train,smote_y_train = smote(robust_X_train, y_train)\n",
        "\n",
        "# adding the generated, augmented data onto the testing data\n",
        "# aug_X_train, new_X_test_data, aug_y_train, new_y_test_data = train_test_split(smote_X_train, smote_y_train, test_size=0.3)\n",
        "# aug_X_test = np.concatenate((robust_X_test, new_X_test_data), axis=0)\n",
        "# aug_y_test = np.concatenate((y_test, new_y_test_data), axis=0)"
      ],
      "metadata": {
        "id": "K5gb5MaVFckP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_smote_X_train = np.expand_dims(smote_X_train, axis=2)\n",
        "cnn_smote_y_train = smote_y_train\n",
        "\n",
        "############\n",
        "\n",
        "cnn_preprocess_X_test = np.expand_dims(robust_X_test, axis=2)\n",
        "cnn_preprocess_y_test = y_test\n",
        "\n",
        "############\n",
        "\n",
        "cnn_X_train = np.expand_dims(X_train, axis=2)\n",
        "cnn_y_train = y_train\n",
        "\n",
        "############\n",
        "\n",
        "cnn_X_test = np.expand_dims(X_test, axis=2)\n",
        "cnn_y_test = y_test\n",
        "\n",
        "############"
      ],
      "metadata": {
        "id": "HVCzUd78Fhk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "\n",
        "# First, initialize model\n",
        "model = Sequential()\n",
        "input_shape = [3197, 1]\n",
        "\n",
        "#######TODO#########\n",
        "\n",
        "model.add(Conv1D(8, 5, activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
        "model.add(Conv1D(16, 3, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "####################\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "kCv6cA3pFwAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "\n",
        "################\n",
        "history = model.fit(cnn_smote_X_train, cnn_smote_y_train, batch_size=64, epochs=20, verbose=1, validation_data=(cnn_preprocess_X_test, cnn_preprocess_y_test), shuffle=True)\n",
        "################"
      ],
      "metadata": {
        "id": "tsVkX5h9G06y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_results(model, cnn_smote_X_train, cnn_smote_y_train, cnn_preprocess_X_test, cnn_preprocess_y_test)"
      ],
      "metadata": {
        "id": "pWj5APZzG91D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}